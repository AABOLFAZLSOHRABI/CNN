# Fire Detection with YOLO V10n Model

Welcome to the Fire Detection project using the YOLO V10n model! This project leverages the latest advances in convolutional neural networks and deep learning to accurately detect fire in images and videos. Whether you are a researcher, developer, or enthusiast in the field of computer vision, this repository provides valuable resources and code to aid in the detection of fire in various environments.

## Overview

Fire detection is a critical application in many fields, including safety, surveillance, and disaster management. Early detection of fire can help prevent widespread damage and save lives. This project utilizes the YOLO V10n model, a state-of-the-art object detection algorithm, to identify fire in real-time.

## YOLO V10n Model

The YOLO (You Only Look Once) V10n model is an advanced version of the popular YOLO object detection framework. Known for its speed and accuracy, YOLO V10n can process images and videos rapidly while maintaining high detection precision. This makes it ideal for applications where real-time fire detection is crucial.

### Key Features of YOLO V10n:

- **High Speed**: Capable of real-time processing.
- **Accuracy**: High precision in detecting objects, including small and partially obscured fires.
- **Efficiency**: Optimized for both performance and computational efficiency.

## Dataset

The model is trained using a comprehensive [fire detection dataset](https://universe.roboflow.com/-jwzpw/continuous_fire/dataset/6) provided by Roboflow. This dataset includes a variety of images depicting fires in different environments and conditions, ensuring that the model can generalize well to real-world scenarios.

### Dataset Details:

- **Variety**: Includes images of fires in different settings, such as urban, forest, and industrial areas.
- **Annotations**: Each image is annotated with bounding boxes to indicate the presence and location of fire.
- **Quality**: High-resolution images to improve model training and accuracy.
